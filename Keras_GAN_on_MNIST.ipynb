{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Generative Adversarial Net experiment on MNIST data\n",
    "------------\n",
    "\n",
    "A basic GAN in Keras\n",
    "\n",
    "Problems: the discriminator model is no good\n",
    "I'm going to try maxout layers and then convolutional next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "import keras\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "train_dataset = mnist.train.images - 0.5\n",
    "train_labels = mnist.train.labels\n",
    "valid_dataset = mnist.validation.images -0.5\n",
    "valid_labels = mnist.validation.labels\n",
    "test_dataset = mnist.test.images - 0.5\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_imagelist_as_grid(img_list, nrow, ncol):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    grid = AxesGrid(fig, 111, nrows_ncols=(nrow, ncol), axes_pad=0.05, label_mode=\"1\")\n",
    "    for i in range(nrow*ncol):\n",
    "        im = grid[i].imshow(img_list[i], interpolation=\"none\", cmap='gray', vmin=-0.5, vmax=0.5)\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "dataset_mean = np.mean(train_dataset)\n",
    "dataset_std = np.std(train_dataset)\n",
    "print(\"mean and std: \", dataset_mean, dataset_std)\n",
    "#show_imagelist_as_grid(train_dataset[:16].reshape(-1,image_size,image_size),4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 24\n",
    "num_steps = 2000\n",
    "dummy_size = 512\n",
    "stddev=0.05\n",
    "dropout_prob_gen = 0.6\n",
    "dropout_prob_discr = 0.6\n",
    "lrelu_alpha = 0.3\n",
    "\n",
    "num_discr_layer1 = dummy_size//2\n",
    "num_discr_layer2 = dummy_size//2\n",
    "num_discr_layer3 = dummy_size//2\n",
    "\n",
    "# generator: 3 hidden layers\n",
    "num_gen_input_size = dummy_size\n",
    "num_gen_layer1 = dummy_size*2\n",
    "num_gen_layer2 = dummy_size*2\n",
    "num_gen_layer3 = dummy_size*2\n",
    "\n",
    "def gen_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_gen_layer1, input_shape=(num_gen_input_size,), init='normal'))\n",
    "    model.add(Dropout(dropout_prob_gen))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(num_gen_layer2, init='normal'))\n",
    "    model.add(Dropout(dropout_prob_gen))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(num_gen_layer3, init='normal'))\n",
    "    model.add(Dropout(dropout_prob_gen))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(image_size*image_size, init='normal'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "def discr_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_discr_layer1, input_shape=(image_size*image_size,), init='normal'))\n",
    "    model.add(Dropout(dropout_prob_discr))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(num_discr_layer2, init='normal'))\n",
    "    model.add(Dropout(dropout_prob_discr))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(num_discr_layer3, init='normal'))\n",
    "    model.add(Dropout(dropout_prob_discr))\n",
    "    model.add(LeakyReLU(alpha=lrelu_alpha))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def discr_on_gen_model(gen, discr):\n",
    "    model = Sequential()\n",
    "    model.add(gen)\n",
    "    discr.trainable = False\n",
    "    model.add(discr)\n",
    "    return model\n",
    "    \n",
    "discriminator = discr_model();\n",
    "generator = gen_model();\n",
    "discriminator_on_generator = discr_on_gen_model(generator, discriminator);\n",
    "\n",
    "discr_optimizer = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "gen_optimizer = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "\n",
    "generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "discriminator_on_generator.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "discriminator.trainable = True # we had made it untrainable when we added to the discriminator_on_generator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=discr_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "print_step = 100\n",
    "\n",
    "gen_loss_total = 0.0\n",
    "gen_trained = 0\n",
    "discr_loss_total = 0.0\n",
    "discr_trained = 0\n",
    "\n",
    "batch_1s = np.zeros(batch_size)+1\n",
    "batch_0s = np.zeros(batch_size)\n",
    "\n",
    "\n",
    "print(\"Starting training.\")\n",
    "for step in range(num_steps):\n",
    "\n",
    "    #train discriminator\n",
    "    offset = (round(random.uniform(0, 100000)) + step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_noise = np.random.normal(loc=dataset_mean, scale=dataset_std, size=(batch_size, num_gen_input_size))\n",
    "    generated_images = generator.predict(batch_noise, verbose=0)\n",
    "    # batch_X = np.concatenate((batch_data, generated_images))\n",
    "    # batch_Y = np.concatenate((np.zeros(batch_size)+1, np.zeros(batch_size)))\n",
    "    discriminator.trainable = True # just to make sure it's still trainable\n",
    "    ld1 = discriminator.train_on_batch(batch_data, batch_1s)\n",
    "    ld2 = discriminator.train_on_batch(generated_images, batch_0s)\n",
    "    discr_loss_total += (ld1+ld2)/2\n",
    "    discr_trained += 1\n",
    "    \n",
    "    #train generator\n",
    "    batch_noise = np.random.normal(loc=dataset_mean, scale=dataset_std, size=(batch_size, num_gen_input_size))    \n",
    "    discriminator.trainable = False # make sure we train only the generator\n",
    "    lg = discriminator_on_generator.train_on_batch(batch_noise, np.zeros(batch_size) + 1)\n",
    "    discriminator.trainable = True # just to make sure it's still trainable\n",
    "    gen_loss_total += lg\n",
    "    gen_trained += 1\n",
    "        \n",
    "    if (step % print_step == print_step-1):\n",
    "        if discr_trained == 0:\n",
    "            discr_trained = 1\n",
    "        print('Minibatch loss before step %d: discriminator %f, generator: %f' % (step+1, discr_loss_total/discr_trained, gen_loss_total/gen_trained))\n",
    "        gen_loss_total = 0.0\n",
    "        discr_loss_total = 0.0\n",
    "        gen_trained = 0\n",
    "        discr_trained = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_noise = np.random.normal(loc=dataset_mean, scale=dataset_std, size=(batch_size, num_gen_input_size))    \n",
    "generated_images = generator.predict(batch_noise, verbose=0)\n",
    "show_imagelist_as_grid(generated_images.reshape(-1,image_size,image_size), 4,4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
